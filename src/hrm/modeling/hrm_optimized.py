"""
Optimized HRM - Smart parameter allocation for <500K params, >40% accuracy.

Key optimizations:
1. Single efficient attention (not multi-scale) - saves 65K params
2. Simple embeddings (not hierarchical) - saves 5K params  
3. Shared transformer blocks between high/low levels - saves 65K params
4. Simple residual connections (not gated fusion) - saves 48K params
5. Reallocate saved ~180K params â†’ larger hidden size (96 instead of 64)

Result: Same 500K budget, 50% more capacity in hidden size!
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from dataclasses import dataclass


@dataclass
class OptimizedConfig:
    max_seq_len: int = 50
    vocab_size: int = 1200
    hidden_size: int = 96  # Increased from 64!
    num_layers: int = 3  # More depth
    num_heads: int = 6
    expansion: float = 2.5
    high_level_cycles: int = 2
    low_level_cycles: int = 1  # Reduced
    use_features: bool = True
    num_users: int = 182
    dropout: float = 0.1  # Less dropout for small data
    recency_decay: float = 0.85


class CompactTemporalEmbedding(nn.Module):
    """Efficient temporal embedding."""
    
    def __init__(self, hidden_size: int):
        super().__init__()
        # Use smaller dimensions and single MLP
        self.hour_embed = nn.Embedding(24, 12)
        self.weekday_embed = nn.Embedding(7, 8)
        # 12 (hour) + 8 (weekday) + 1 (minute_norm) = 21
        self.mlp = nn.Sequential(
            nn.Linear(21, hidden_size),
            nn.GELU()
        )
    
    def forward(self, hour, minute, weekday):
        # Normalize minute to [0,1] as continuous feature
        minute_norm = (minute.float() / 60.0).unsqueeze(-1)
        h_emb = self.hour_embed(hour)
        w_emb = self.weekday_embed(weekday)
        combined = torch.cat([h_emb, w_emb, minute_norm], dim=-1)
        return self.mlp(combined)


class EfficientAttention(nn.Module):
    """Single-path attention with recency bias."""
    
    def __init__(self, hidden_size: int, num_heads: int, recency_decay: float, dropout: float = 0.1):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        self.recency_decay = recency_decay
        
        # Single QKV projection for efficiency
        self.qkv = nn.Linear(hidden_size, hidden_size * 3, bias=False)
        self.out_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, mask=None):
        B, L, _ = x.shape
        
        # Single QKV projection
        qkv = self.qkv(x).reshape(B, L, 3, self.num_heads, self.head_dim)
        q, k, v = qkv.permute(2, 0, 3, 1, 4)
        
        # Scaled dot-product attention
        attn = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        # Add recency bias
        pos = torch.arange(L, device=x.device)
        bias = torch.where(
            pos.unsqueeze(0) - pos.unsqueeze(1) <= 0,
            (pos.unsqueeze(0) - pos.unsqueeze(1)).float() * math.log(self.recency_decay),
            torch.tensor(-1e9, device=x.device)
        )
        attn = attn + bias.unsqueeze(0).unsqueeze(0)
        
        if mask is not None:
            attn = attn.masked_fill(~mask.unsqueeze(1).unsqueeze(2), -1e9)
        
        attn = F.softmax(attn, dim=-1)
        attn = self.dropout(attn)
        
        out = torch.matmul(attn, v).transpose(1, 2).reshape(B, L, self.hidden_size)
        return self.out_proj(out)


class SharedTransformerBlock(nn.Module):
    """Shared transformer block - used by both high and low level reasoning."""
    
    def __init__(self, config: OptimizedConfig):
        super().__init__()
        self.ln1 = nn.LayerNorm(config.hidden_size)
        self.attention = EfficientAttention(
            config.hidden_size,
            config.num_heads,
            config.recency_decay,
            config.dropout
        )
        
        self.ln2 = nn.LayerNorm(config.hidden_size)
        hidden_dim = int(config.hidden_size * config.expansion)
        self.ffn = nn.Sequential(
            nn.Linear(config.hidden_size, hidden_dim),
            nn.GELU(),
            nn.Dropout(config.dropout),
            nn.Linear(hidden_dim, config.hidden_size),
            nn.Dropout(config.dropout)
        )
    
    def forward(self, x, mask=None):
        # Pre-norm architecture
        x = x + self.attention(self.ln1(x), mask)
        x = x + self.ffn(self.ln2(x))
        return x


class CompactCrossAttention(nn.Module):
    """Compact cross-attention."""
    
    def __init__(self, hidden_size: int, num_heads: int, dropout: float = 0.1):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        
        self.q_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.kv_proj = nn.Linear(hidden_size, hidden_size * 2, bias=False)
        self.out_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, query, key_value):
        B, QL, _ = query.shape
        KL = key_value.shape[1]
        
        q = self.q_proj(query).view(B, QL, self.num_heads, self.head_dim).transpose(1, 2)
        kv = self.kv_proj(key_value).reshape(B, KL, 2, self.num_heads, self.head_dim)
        k, v = kv.permute(2, 0, 3, 1, 4)
        
        attn = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        attn = F.softmax(attn, dim=-1)
        attn = self.dropout(attn)
        
        out = torch.matmul(attn, v).transpose(1, 2).reshape(B, QL, self.hidden_size)
        return self.out_proj(out)


class OptimizedHRM(nn.Module):
    """
    Optimized HRM with smart parameter allocation.
    Target: <500K params, >40% test accuracy.
    """
    
    def __init__(self, config: OptimizedConfig, device='cuda'):
        super().__init__()
        self.config = config
        self.device = device
        
        # Simple embeddings
        self.location_embed = nn.Embedding(config.vocab_size, config.hidden_size)
        self.temporal_embed = CompactTemporalEmbedding(config.hidden_size)
        
        if config.use_features:
            self.user_embed = nn.Embedding(config.num_users, config.hidden_size // 4)
        self.duration_proj = nn.Linear(1, config.hidden_size // 4)
        
        # Input projection
        input_dim = config.hidden_size * 2  # loc + temporal
        if config.use_features:
            input_dim += config.hidden_size // 2
        self.input_proj = nn.Linear(input_dim, config.hidden_size)
        
        # Reasoning state
        self.reasoning_init = nn.Parameter(torch.randn(1, 1, config.hidden_size) * 0.02)
        
        # SHARED transformer blocks (key optimization!)
        self.shared_blocks = nn.ModuleList([
            SharedTransformerBlock(config) for _ in range(config.num_layers)
        ])
        
        # Compact cross-attention
        self.cross_attention = CompactCrossAttention(config.hidden_size, config.num_heads, config.dropout)
        
        # Simple residual instead of gated fusion
        self.reasoning_mix = nn.Linear(config.hidden_size * 2, config.hidden_size)
        
        # Output projection with layer norm
        self.output_ln = nn.LayerNorm(config.hidden_size)
        self.output_head = nn.Linear(config.hidden_size, config.vocab_size)
        
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.ones_(module.weight)
            torch.nn.init.zeros_(module.bias)
    
    def forward(self, location_ids, features=None):
        B, L = location_ids.shape
        
        # Embeddings
        loc_emb = self.location_embed(location_ids)
        
        if features is not None and self.config.use_features:
            start_min = features['start_min'].long()
            hour = torch.div(start_min, 60, rounding_mode='floor') % 24
            minute = start_min % 60
            weekday = features['weekday']
            
            temporal_emb = self.temporal_embed(hour, minute, weekday)
            user_id = features['user'][:, 0]
            user_emb = self.user_embed(user_id).unsqueeze(1).expand(-1, L, -1)
            duration = features['duration'].unsqueeze(-1)
            duration_emb = self.duration_proj(duration)
            
            combined = torch.cat([loc_emb, temporal_emb, user_emb, duration_emb], dim=-1)
        else:
            combined = loc_emb
        
        # Input projection
        x = self.input_proj(combined)
        reasoning_state = self.reasoning_init.expand(B, 1, -1)
        
        # Hierarchical reasoning with shared blocks
        for _ in range(self.config.high_level_cycles):
            # High-level: process sequence
            for block in self.shared_blocks:
                x = block(x)
            
            # Low-level: refine reasoning (fewer cycles)
            for _ in range(self.config.low_level_cycles):
                for block in self.shared_blocks:
                    reasoning_state = block(reasoning_state)
                
                # Cross-attend to history
                cross_out = self.cross_attention(reasoning_state, x)
                
                # Simple residual mix instead of gated fusion
                reasoning_state = self.reasoning_mix(
                    torch.cat([reasoning_state, cross_out], dim=-1)
                )
            
            # Update last position with reasoning
            last_pos = x[:, -1:, :]
            updated = self.reasoning_mix(torch.cat([last_pos, reasoning_state], dim=-1))
            x = torch.cat([x[:, :-1, :], updated], dim=1)
        
        # Output
        logits = self.output_head(self.output_ln(reasoning_state.squeeze(1)))
        return logits


def count_parameters(model):
    total = sum(p.numel() for p in model.parameters())
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return total, trainable
